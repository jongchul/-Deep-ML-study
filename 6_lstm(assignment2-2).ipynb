{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 729\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "#  print('predictions is %s' % predictions)\n",
    "#  print('labels is %s' % labels)\n",
    "#  print('label shape is %s' % str(labels.shape))\n",
    "#  print('labels[0] shape is %s' % labels[0])\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "\n",
    "def chars2eid(chars):\n",
    "  return char2id(chars[0]) * 27 + char2id(chars[1])\n",
    "\n",
    "def eid2chars(eid):\n",
    "  if eid > 0:\n",
    "    return id2char(eid // 27) + id2char(eid % 27)\n",
    "  else:\n",
    "    return '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "####################################################################################################\n",
      "11\n",
      "####################################################################################################\n",
      "[  2 599  22 549 545 139 109  14 154   5 411  23 221 384 495   4 518 324\n",
      " 548 356 382 378  46 587 495   3   1 548 149   9   1 162  13 135   1 521\n",
      "  90   3  47 258  15  41   6 356  15 522  20 153  43 513 418 250   2 419\n",
      " 405 257  46 518 149 533  15 109  18 694]\n",
      "[  2 599  22 549 545 139 109  14 154   5 411  23 221 384 495   4 518 324\n",
      " 548 356 382 378  46 587 495   3   1 548 149   9   1 162  13 135   1 521\n",
      "  90   3  47 258  15  41   6 356  15 522  20 153  43 513 418 250   2 419\n",
      " 405 257  46 518 149 533  15 109  18 694]\n",
      "[139 221 561 549 411 154 397 135 149 236  46   9   4 149 423 324 217 487\n",
      " 135 555  36 167 139 221 540 393 258 423  20 345  20 516 225 383   4  11\n",
      "  18 432 397 411 424 333 324 217 263 398 450 137   9 444 411 349 513 555\n",
      " 385 221  81 139 108  46 548 366 491 360]\n",
      "####################################################################################################\n",
      "2\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "599\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['as', ' t', 'ir', 'na', 'r ', ' d', 'ea', 'in', 'id', 'ig', ' b', 'as', 'ir', 'lu', 't ', 'ea', 'y ', 'st', 're', 's ', ' g', 'ef', 'ur', ' s', 'en', 'an', ' p', 'e ', 'ts', 'nc', 'nd', 'di', 'ac', 'ni', 'll', 'in', 'al', 'ro', 'io', 'n ', 'pp', 'd ', 'ul', ' t', 'f ', 'de', 'h ', ' d', 'er', 'ex', 'y ', 'ht', 'os', 's ', 'co', ' t', 'si', 'nt', 'te', ' p', 'n ', 'y ', 'ep', 'te']\n",
      "[' b', 've', ' v', 'ti', 'te', 'ed', 'da', ' n', 'es', ' e', 'of', ' w', 'he', 'nf', 'ri', ' d', 'se', 'l ', 'th', 'me', 'nd', 'n ', 'as', 'ut', 'ri', ' c', ' a', 'th', 'en', ' i', ' a', 'f ', ' m', 'e ', ' a', 'sh', 'ci', ' c', 'at', 'io', ' o', 'an', ' f', 'me', ' o', 'si', ' t', 'er', 'ap', 's ', 'om', 'ig', ' b', 'on', 'o ', 'in', 'as', 'se', 'en', 'st', ' o', 'da', ' r', 'ys']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "#vocabulary_size = 128   # since we are now projecting our 729 to 128\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size / batch_size\n",
    "        self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        self._last_label = self._next_batch_label()\n",
    "  \n",
    "    \n",
    "#    def _next_batch(self):\n",
    "#        batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "#        for b in range(self._batch_size):\n",
    "#          batch[b] = char2id(self._text[self._cursor[b]])\n",
    "#          self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "#        return batch\n",
    "    \n",
    "        \n",
    "    def _next_batch(self):\n",
    "  \n",
    "        batch = np.zeros(shape=(self._batch_size), dtype = np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "          c = self._cursor[b]\n",
    "#          batch[b] = chars2eid(self._text[c:c+2])\n",
    "          bigram = chars2eid(self._text[c:c+2])\n",
    "          batch[b] = bigram\n",
    "          self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "#    def _next_batch_label(self):\n",
    "#        l = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "#        for b in range(self._batch_size):\n",
    "#          print('^^^^^^^^')\n",
    "#          print(self._text[self._cursor[b]])\n",
    "#          print('==========')\n",
    "#          print(self._text[self._cursor[b] : (self._cursor[b] + 2)])\n",
    "#          l[b, chars2eid(self._text[self._cursor[b] : self._cursor[b] + 2])] = 1.0\n",
    "#        return l\n",
    "    \n",
    "    def _next_batch_label(self):\n",
    "  \n",
    "        label = np.zeros(shape=(self._batch_size,vocabulary_size), dtype = np.int64)\n",
    "        for b in range(self._batch_size):\n",
    "          c = self._cursor[b]\n",
    "#          batch[b] = chars2eid(self._text[c:c+2])\n",
    "          bigram = chars2eid(self._text[c:c+2])\n",
    "          label[b, bigram] = 1.0\n",
    "          self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return label\n",
    "  \n",
    "    def next(self):\n",
    "#        batches = []\n",
    "#        labels = []\n",
    "        batches = [self._last_batch]\n",
    "        labels = [self._last_label]\n",
    "        for step in range(self._num_unrollings):\n",
    "          batches.append(self._next_batch())\n",
    "          labels.append(self._next_batch_label())\n",
    "        \n",
    "        self._last_batch = batches[-1]\n",
    "        self._last_label = labels[-1]\n",
    "        return (batches, labels)\n",
    "    \n",
    "def batches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "\n",
    "    for b in batches:\n",
    "   \n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "\n",
    "    return s\n",
    "\n",
    "def characters(probabilities):\n",
    " \n",
    "    return [eid2chars(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    " \n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "test,test_label = train_batches.next()\n",
    "\n",
    "print(len(test))\n",
    "#print(test_label)\n",
    "print('#' * 100)\n",
    "#print(test[1])\n",
    "#print(test_label[0][0])\n",
    "#print(test_label[0][1])\n",
    "\n",
    "test2,test_label2 = train_batches.next()\n",
    "\n",
    "print(len(test2))\n",
    "print('#' * 100)\n",
    "print(test[10])\n",
    "print(test2[0])\n",
    "print(test2[1])\n",
    "\n",
    "print('#' * 100)\n",
    "print(test2[0][0])\n",
    "print(test_label2[0][0])\n",
    "print(test2[0][1])\n",
    "print(test_label2[0][1])\n",
    "print(characters(test_label2[0]))\n",
    "print([eid2chars(id) for id in test2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size = 729\n",
    "embedding_size = 128\n",
    "totalGate = 4\n",
    "\n",
    "forgetGateIndex = 0\n",
    "inputGateIndex = 1\n",
    "updateGateIndex = 2\n",
    "outputGateIndex = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))            \n",
    "  gateWeightInput = tf.Variable(tf.truncated_normal([totalGate, embedding_size , num_nodes] , -0.1 , 0.1));\n",
    "  gateWeightOutput= tf.Variable(tf.truncated_normal([totalGate, num_nodes , num_nodes] , -0.1 , 0.1));\n",
    "  gateBias = tf.Variable(tf.zeros([totalGate , 1, num_nodes]));\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "#  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  def lstm_cell(i, o, state):\n",
    "        \n",
    "    stackedInput = tf.pack([tf.nn.embedding_lookup(gateWeightInput[g,:,:],i) for g in range(totalGate)])\n",
    "    \n",
    "    stackedOutput = tf.pack([o for _ in range(totalGate)])\n",
    "    \n",
    "    stackedAll = stackedInput+tf.batch_matmul(stackedOutput,gateWeightOutput)+gateBias\n",
    "        \n",
    "    listOfGates = tf.unpack(stackedAll)\n",
    "\n",
    "    forgetGate = tf.sigmoid(listOfGates[forgetGateIndex])\n",
    "    inputGate = tf.sigmoid(listOfGates[inputGateIndex])\n",
    "    updateGate = tf.tanh(listOfGates[updateGateIndex])\n",
    "    outputGate = tf.sigmoid(listOfGates[outputGateIndex])\n",
    "    \n",
    "    state = forgetGate * state + inputGate * updateGate    \n",
    "    \n",
    "    return outputGate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "  \n",
    "  train_inputs = train_inputs[:num_unrollings]\n",
    "  train_labels = train_labels[1:]  # labels are inputs shifted by one time step.\n",
    "                                                        \n",
    "                                                        \n",
    "    \n",
    "#  train_data = list()\n",
    "#  for _ in range(num_unrollings + 1):\n",
    "#    train_data.append(\n",
    "#      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "#  train_inputs = train_data[:num_unrollings]\n",
    "#  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    \n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "#  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591376 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 728.78\n",
      "================================================================================\n",
      "e \n",
      "[135]\n",
      "(1,)\n",
      "e pgbvxbwukefrz slrfidlld fviuuwcrkhkzdbzorcgnhdiizqdlramutsrcnblcjccdcgzhzhjsyljmsffbrpcyoxqmddzn nanqidwsmdqgerkguleg dqm espjbydhoapcrwsvkesudwpm wnaqlguercb\n",
      "qn\n",
      "[473]\n",
      "(1,)\n",
      "qnstgnxopmjtq xvzw  izwzrnibesnjiwqkiejyfehyckvzfjnhmy kqdanrcrkpqvsfdzpnzhxsfwhzzv dereviabqtoblmsexoxjtkakdy gqasookpbzitdelrogrgurlrkacwkphwipxkyud bnncnssxj\n",
      "pg\n",
      "[439]\n",
      "(1,)\n",
      "pgpiedarfjxxethkrlakpdizetraxplhmw yyvaobtiwlowozpiseuyhgxyhkskgyuonghtsvvovmqngxuniqvvrovojomlolqlmueymvblmlasindtyayywcidcttuplwsgsey reobfbv xhftsrapljp bfra\n",
      "uw\n",
      "[590]\n",
      "(1,)\n",
      "uwrgs ts tyagbbwauawhlvupxuilzucmbgnsyyopvmbaolpckdqryozxztoekhmgvifmsjtqfsuahsxkwfvapmuvhujdamrpggcydonsehvylmnxjufwve huusauunaywuxyarwz o eiecjrfselkjruf ifu\n",
      "zb\n",
      "[704]\n",
      "(1,)\n",
      "zbuyppzkcckyldmmwdgizfdyfkclpnrtonakmzhkrmgjnxcneeorc mhncpytxmnt cmfm rocp pbogvdzd t mznzroexplynkypummjggnirnuwhlgmec lujranihccnhrysmmgyrtmzuiurzlwhzclvikei\n",
      "================================================================================\n",
      "Average loss at step 100: 5.433508 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 201.26\n",
      "Average loss at step 200: 5.282398 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 192.62\n",
      "Average loss at step 300: 5.279879 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 193.12\n",
      "Average loss at step 400: 5.260970 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 192.93\n",
      "Average loss at step 500: 5.265600 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 187.28\n",
      "Average loss at step 600: 5.262868 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 182.76\n",
      "Average loss at step 700: 5.251192 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 191.12\n",
      "Average loss at step 800: 5.248431 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 197.36\n",
      "Average loss at step 900: 5.244334 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 195.26\n",
      "Average loss at step 1000: 5.235480 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.04\n",
      "================================================================================\n",
      "tz\n",
      "[566]\n",
      "(1,)\n",
      "tzwyveilwaun dy la ta y newhhrio r tede ks sn t s to proetsts  igakge e  ws chbolfbrinevrer ofvarerid enpxkey exthththinyoa a e asdeal f sakl eslih aiole roalra\n",
      "pe\n",
      "[437]\n",
      "(1,)\n",
      "pewhliacsiigpofut itrg ireanrpits  n iowtsa fie e ran s ea mthed ee cf fe mph ert l tefflebyietl ne tha cao  dhinol minwtoy i inaritesh ieitiskedwouus i ernn oo\n",
      "yn\n",
      "[689]\n",
      "(1,)\n",
      "ynryneerhoat tti theegfot ved r rarotht tavit anryfr ith l py fom  eer oarthst tan wd edthne ay eve ltes w tzson wuaide eongroatoncke nsouthiathislangaievx coni\n",
      "iv\n",
      "[265]\n",
      "(1,)\n",
      "ivxo f ny e thneecra bdsn y  ee eekny  mivel sam cntsuammed baen icoanan ttysek las anjuorisr ine al pheheg  btheyct thoednty lkwo mora  we iot ure unedinren ti\n",
      "qf\n",
      "[465]\n",
      "(1,)\n",
      "qfat tviol c boferbeth sovonivtha he bindia ouageeed frr tanci jfumahaers e  salfo ng  ulellmoy ou n sonnoa tirrsfouves l ved grl stceherionzeire ec indipurn y \n",
      "================================================================================\n",
      "Average loss at step 1100: 5.239069 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 183.59\n",
      "Average loss at step 1200: 5.234895 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 198.95\n",
      "Average loss at step 1300: 5.198581 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 183.59\n",
      "Average loss at step 1400: 5.204217 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 175.21\n",
      "Average loss at step 1500: 5.223781 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 186.88\n",
      "Average loss at step 1600: 5.208744 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 193.83\n",
      "Average loss at step 1700: 5.229422 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.09\n",
      "Average loss at step 1800: 5.212536 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 174.34\n",
      "Average loss at step 1900: 5.217853 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 191.88\n",
      "Average loss at step 2000: 5.199956 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 186.65\n",
      "================================================================================\n",
      "ap\n",
      "[43]\n",
      "(1,)\n",
      "ap rcoennet  arseee  ointuzjlde afhee d tes trths  k be gamaatonomtoradislndiehee hohebiin gve lenest de sg liy  brdtiryea mr  tt er aheakennaon n wanldin isebe\n",
      "da\n",
      "[109]\n",
      "(1,)\n",
      "da a rer tro mstars  rumacmeron ac iive ad troidveindesitrets uileedhang t aof bsto ag m tioic jeuelin tc  ss  uanoranannelovendo erale mein ey  win zboh shytja\n",
      "kh\n",
      "[305]\n",
      "(1,)\n",
      "khapse dghrosiar tadacchheysmie ushehehtde fst zd y maney deripiha eerelorwir openez rtiidheecrewie loen ar relobrs kgs flpore freeelypod  whizrfogrofaltt fwaat\n",
      "zd\n",
      "[706]\n",
      "(1,)\n",
      "zdchiainv odnuap om g ftd m  sacma tgin  wofsugwmetisetaplendustmee d  tloinsege misscteise anrfqire n ctlonciechae iohe tioieogitsty atend juonniorree m tam ti\n",
      "qq\n",
      "[476]\n",
      "(1,)\n",
      "qqumus wrnthd odrevla stun tsas vene fthm ogd reengepente ecwi conghfrs unou wt  temotaswoond rihessncasne ee ltaty is zin a lr e  tveo ree  allontih ca a trngh\n",
      "================================================================================\n",
      "Average loss at step 2100: 5.200016 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 182.81\n",
      "Average loss at step 2200: 5.209897 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.72\n",
      "Average loss at step 2300: 5.201680 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 158.18\n",
      "Average loss at step 2400: 5.186595 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 174.96\n",
      "Average loss at step 2500: 5.187867 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.19\n",
      "Average loss at step 2600: 5.196194 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 190.93\n",
      "Average loss at step 2700: 5.207801 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 179.53\n",
      "Average loss at step 2800: 5.199780 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 188.74\n",
      "Average loss at step 2900: 5.196418 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 188.50\n",
      "Average loss at step 3000: 5.189363 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.81\n",
      "================================================================================\n",
      "oh\n",
      "[413]\n",
      "(1,)\n",
      "oh tewg onersir f e omalomoutoe onn  a sa errton ewivitain tio ros k ta incoieercaaxlaas p zrg hintin ab rhrwefaid ftiinloclnongorchine avd sttahocichnetizeasmp\n",
      "hb\n",
      "[218]\n",
      "(1,)\n",
      "hbwiatapunoie  xftlahee rslyeratni aedrnfttiba ddrafngryojchfsneewarprgl cnnaserd iekio neasoc tonalerhi pju hnss  cabin aete asveno iasnt ailneisetise artrnydi\n",
      "ea\n",
      "[136]\n",
      "(1,)\n",
      "ea pevanavaco ec tisd hayie he ge  feikxniriiooa hia tnepus ieplouanexenigopetontyzemaween carrieron c t bro wiconts w bsen o leifitriss fin aeff  borh inbsou t\n",
      "nd\n",
      "[382]\n",
      "(1,)\n",
      "ndd  e ts  wn th agi bt heel m id ng nframhe ttsinli ie afine minc b oinburmcorkn idgrtef tieschreltskmaefe a  aumlleris fmeveheyod ein yzs hetrom ey atseukovnt\n",
      "gc\n",
      "[192]\n",
      "(1,)\n",
      "gcteemn oympctisescoenra agaotel rres  lisdwonild viti hxld niucon t gd redse no aonviignslae  dfrwhinale  te alaygi flecaesnonetoricoonenseex se x nde s erhewa\n",
      "================================================================================\n",
      "Average loss at step 3100: 5.201972 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 174.35\n",
      "Average loss at step 3200: 5.189635 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 158.34\n",
      "Average loss at step 3300: 5.199168 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 189.63\n",
      "Average loss at step 3400: 5.199863 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 187.57\n",
      "Average loss at step 3500: 5.194778 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 179.47\n",
      "Average loss at step 3600: 5.185439 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 176.64\n",
      "Average loss at step 3700: 5.169137 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 183.99\n",
      "Average loss at step 3800: 5.164845 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 187.00\n",
      "Average loss at step 3900: 5.154225 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.61\n",
      "Average loss at step 4000: 5.163297 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 178.09\n",
      "================================================================================\n",
      "wo\n",
      "[636]\n",
      "(1,)\n",
      "wor wi ber t d srotevawiuaomng phe b rt  wtwligiunlerkinittereo uniay icd  tiotwtidsvih mansme btrrpa m rshetyonans s atfoe ysheerc  eele ncr ina urorante ey iv\n",
      "xb\n",
      "[650]\n",
      "(1,)\n",
      "xbe hehe oca aactwva rim pplastlnt agaand upe inemen ma  han tab i lken llicedthon nred e zibe tla dd stnageelpo er viaspondarviar h esi n wanttusasree haleomiz\n",
      "nz\n",
      "[404]\n",
      "(1,)\n",
      "nzdsrok mo saby ncheinla nur oi stdstuiszawiogs ndrenealesonc  rrg t pr buouina geomedananivs nt wanitsee tyoff synng culaecrendciseoteuofrfemthovpralr  cl  tns\n",
      "nz\n",
      "[404]\n",
      "(1,)\n",
      "nznto thtooutremefe e eoen c l pode ed tnyit nive anhestunouy  aevg ix dtuhe gncy tw i ibemeneefs headcoo foncg inonrooyg nio hiedonngias e id o t tgyalanirbedi\n",
      "pi\n",
      "[441]\n",
      "(1,)\n",
      "pitsgeecras pot nondanthes sngn  iaiinnsulald ve ttines alth cmeos dn ea tvirns  trdoke an hthcl toritorrgigheel tgosoigthioattwupd it wdith t omohan laf agwhfi\n",
      "================================================================================\n",
      "Average loss at step 4100: 5.159514 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 180.01\n",
      "Average loss at step 4200: 5.190557 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 183.37\n",
      "Average loss at step 4300: 5.186941 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.63\n",
      "Average loss at step 4400: 5.176797 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 178.79\n",
      "Average loss at step 4500: 5.164315 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 176.91\n",
      "Average loss at step 4600: 5.177060 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 175.32\n",
      "Average loss at step 4700: 5.168529 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.71\n",
      "Average loss at step 4800: 5.183274 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 162.37\n",
      "Average loss at step 4900: 5.176960 learning rate: 10.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 176.98\n",
      "Average loss at step 5000: 5.149603 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 170.97\n",
      "================================================================================\n",
      "rr\n",
      "[504]\n",
      "(1,)\n",
      "rrice  onialn n h rsw  funs whe in iy peanan rate pereod es s  ao deloo adgaecs wiro bwoseoos ne in ilcaho simwa w pmani z ondesloe to t celasbenvheroise kyraio\n",
      "or\n",
      "[423]\n",
      "(1,)\n",
      "orso flepu cten  wavke o ger che adual barheisteat gsccoysamer n ae nianredvnihachominlsn  ae errs od atf au tan at wiudin oitecteysioomviinatdeamwae rel h laof\n",
      "hh\n",
      "[224]\n",
      "(1,)\n",
      "hhooeme  io himefeabvela t c bnsthveurth o tvin  pryrealafworoener cisenanneans baapicamors onthalres utwiant oitlfonfe erhecieftaied li zibe  ah erndisiontashi\n",
      "ws\n",
      "[640]\n",
      "(1,)\n",
      "wspota oert flluatn rgerl e ee vicn  bmoju tisovenbodyunne p ne  t f oiealtos su aurhor  tasmankdethe enr thm opixs so tri fcoseheveurfo cre l atece nhaa naheaf\n",
      "um\n",
      "[580]\n",
      "(1,)\n",
      "umagedn m teaie irh  me rredh ontss  ut n gaols  bet ml teanveouncveouar ies ooffrcors w adsrowoisnte ertb hy hema nch dia spps ssessoeddoesguimti thelaqpcekeab\n",
      "================================================================================\n",
      "Average loss at step 5100: 5.166908 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 175.18\n",
      "Average loss at step 5200: 5.153443 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 170.87\n",
      "Average loss at step 5300: 5.158396 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 184.25\n",
      "Average loss at step 5400: 5.160454 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 178.63\n",
      "Average loss at step 5500: 5.163917 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 177.60\n",
      "Average loss at step 5600: 5.159104 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 158.17\n",
      "Average loss at step 5700: 5.162276 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 183.71\n",
      "Average loss at step 5800: 5.166959 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 168.37\n",
      "Average loss at step 5900: 5.156058 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 155.56\n",
      "Average loss at step 6000: 5.153392 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 168.03\n",
      "================================================================================\n",
      "li\n",
      "[333]\n",
      "(1,)\n",
      "lit in aenemolrgte sngonstintacaenurnenu oiccorihe ter wdeennyi founstd y hrhat  tneogtwveedurr s e d z urre aenthssceiss taxi tfantomiv wanal schfulehe sstmeti\n",
      "wm\n",
      "[634]\n",
      "(1,)\n",
      "wmn tsnsnd wthstorg imgh oe zae  sss aest resestinono eal xtr  i talniplh d eruaere noisoftewhss a rncl zerethd anhuaralkifoncnel  ptendbln mun t ngct o tonffsu\n",
      "bx\n",
      "[78]\n",
      "(1,)\n",
      "bxhlf cagaedn  t tivoullbu monvetaalatonsp ueaeransoirrineli tr entacumeo  biccr otie o e noocr  chaarincirive irsaqricithd s ngf ecn eet twe cititi wgasttineo \n",
      "iq\n",
      "[260]\n",
      "(1,)\n",
      "iq oedneeseronarwo tthchlj aanthmaathee gaalos aewoctwincoenra carhas rdndlaer fhelibuarmescouf t grs leh  ouminel leleroratsp d ts lotrt ayn  cs neths e eradhi\n",
      " f\n",
      "[6]\n",
      "(1,)\n",
      " ffangeswoiar irheofmoexut atwzes n  monupnione e w weozha ewezefiniseonavzefi oaoeinithth so mazestsezesithzeganitwzeb  bhl isetw inignonresetwseniensizealzeer\n",
      "================================================================================\n",
      "Average loss at step 6100: 5.159074 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 164.03\n",
      "Average loss at step 6200: 5.153061 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 176.13\n",
      "Average loss at step 6300: 5.161494 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 185.77\n",
      "Average loss at step 6400: 5.183506 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 175.31\n",
      "Average loss at step 6500: 5.147784 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 163.31\n",
      "Average loss at step 6600: 5.160320 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 167.70\n",
      "Average loss at step 6700: 5.137530 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 186.70\n",
      "Average loss at step 6800: 5.161796 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 181.91\n",
      "Average loss at step 6900: 5.136722 learning rate: 1.000000\n",
      "[[0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 169.90\n",
      "Average loss at step 7000: 5.152727 learning rate: 1.000000\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(640, 729)\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 169.41\n",
      "================================================================================\n",
      "ae\n",
      "[32]\n",
      "(1,)\n",
      "aed ba taiexnain serl oroytrcsf  phi athndon oarerd sphe po hiidfo cnttw f thalybnesios mmce eon cn se wa us aitptwiipl s kncee iger immonons anisaritra wde ico\n",
      "ia\n",
      "[244]\n",
      "(1,)\n",
      "iaayto autt e  tlyitsuesinrmtaal sonalch apt a onees tunmeicya mt s g onomicli cmiffssh olfashmielngde co ayreceooanzeanmude zy usitn wit k eg biempt s ush  aoo\n",
      "fz\n",
      "[188]\n",
      "(1,)\n",
      "fzd essks th oe erziel tunich thd viico  oroanmis pspaphntonsit opmoouulin hth t ae ecyshuittonge loomane ththrbfebl if etgtn  iunn onfecsd s nchewafih rend aim\n",
      "ai\n",
      "[36]\n",
      "(1,)\n",
      "aid ibn chd apboea t m barntrewadas r or tpet e lddubyhyibor pers su rths caspstho n pagcaerru ms gattcecarantloarcrtel nfllht gold atoowortthorrasiriisivsod ct\n",
      "qq\n",
      "[476]\n",
      "(1,)\n",
      "qq anua ar k aunthau welusssnde d ont  und pin andn mmhe kghd hi cusfounann ixbierhe terom meravy le carganousn a lld g s sifite fciomarary  drlmsthrdjelandstoj\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):  \n",
    "    (batches, labels) = train_batches.next()\n",
    "    \n",
    "    feed_dict = dict()\n",
    "   \n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "      feed_dict[train_labels[i]] = labels[i+1]     \n",
    "#      feed_dict[train_labels[i]] = labels[i]   \n",
    "    batches = train_batches.next()\n",
    " #   print(batches)\n",
    " #   feed_dict = dict()\n",
    " #   for i in range(num_unrollings + 1):\n",
    " #     feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "#        print(predictions.shape)\n",
    "#        print(predictions)\n",
    "#        print(labels.shape)\n",
    "#        print(labels)\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "#      labels = np.concatenate(list(batches)[1:])\n",
    "      labels = np.concatenate(list(labels)[1:])\n",
    "      print(labels)\n",
    "      print(labels.shape)\n",
    "      print('prediction: %s' % str(predictions.shape))  \n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "      \n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          print(sentence)\n",
    "#          sentence = characters(feed)[0]\n",
    "          feed2 = np.array([chars2eid(sentence)])\n",
    "          print(feed2)\n",
    "          print(feed2.shape)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed2})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "#      reset_sample_state.run()\n",
    "#      valid_logprob = 0\n",
    "#      for _ in range(valid_size):\n",
    "#        b = valid_batches.next()\n",
    "#        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "#        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "#      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "#        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
