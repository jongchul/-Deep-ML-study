{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batches_text_size 99999000 \n",
      "train_batches_cursor [1, 1562485, 3124969, 4687453, 6249937, 7812421, 9374905, 10937389, 12499873, 14062357, 15624841, 17187325, 18749809, 20312293, 21874777, 23437261, 24999745, 26562229, 28124713, 29687197, 31249681, 32812165, 34374649, 35937133, 37499617, 39062101, 40624585, 42187069, 43749553, 45312037, 46874521, 48437005, 49999489, 51561973, 53124457, 54686941, 56249425, 57811909, 59374393, 60936877, 62499361, 64061845, 65624329, 67186813, 68749297, 70311781, 71874265, 73436749, 74999233, 76561717, 78124201, 79686685, 81249169, 82811653, 84374137, 85936621, 87499105, 89061589, 90624073, 92186557, 93749041, 95311525, 96874009, 98436493] \n",
      "train_batches_cursor length 64 \n",
      "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "#      print(\"cursor %s\" % self._cursor[b])\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "#      print(\"after cursor %s\" % self._cursor[b])\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "#  for c in np.argmax(probabilities, 1):\n",
    "#        print(\"c is %s \" % c)\n",
    "\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "\n",
    "#  print(batches)\n",
    "\n",
    "  s = [''] * batches[0].shape[0]\n",
    "\n",
    "#  print('s is %s ' % s)\n",
    "\n",
    "  for b in batches:\n",
    "   \n",
    " #  if (len(batches) > 2):\n",
    " #   print(\"batches size %s \" % len(batches))\n",
    " #   print(\"b in batches %s \" % b[63,24])\n",
    " \n",
    "\n",
    "   s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#    s = characters(b)\n",
    "\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "print(\"train_batches_text_size %s \" % train_batches._text_size)\n",
    "print(\"train_batches_cursor %s \" % train_batches._cursor)\n",
    "print(\"train_batches_cursor length %s \" % len(train_batches._cursor))\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "test = train_batches.next()\n",
    "print(test)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295162 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "wurm m huledreyvud f eccennibttodusdh dtedigsd   gywlmb  dnqnekwmcxkiyc niz efft\n",
      "ugusxegeh on otee roi eazdcldi kzfpt  fkgiwz lrahbqaxfw eyovaotv enfexyeieknvlel\n",
      "nmtduomt blsd sj udozrhyslnonsyeagmoarw  taaeeyiuzyeatxlkuwc ioeyvixienhoiey ino\n",
      "kp fzxres shsvnucenz  xa so bxtscvv eokljfiahk yropwottpzp nror wpp  omxvncb bix\n",
      "jexl  flkdgsndanrrepheuccau ze qiild  wjewupyn fnazrzp dgeokez cihsioriugw vt ee\n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.581988 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.53\n",
      "Validation set perplexity: 10.62\n",
      "Average loss at step 200: 2.230330 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 9.60\n",
      "Average loss at step 300: 2.072000 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 400: 1.989246 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.991086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 600: 1.918038 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.890035 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 800: 1.867946 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 900: 1.854970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.787850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "h while fullity and or britinu of spebsen shigisnike to gisctional not is apquel\n",
      "e intuldsborth the pa quiting on ndwmlies unut havaglis or introm is cred submic\n",
      "chaly returtery other a renotensity it maning that cas bulint contrae the eight \n",
      "fs in wort rigles with condeciakca was not grectional siltides marabchory or nam\n",
      "ond one hin g hand a commod thit reberted symatametic inplisedt cipaunterent mil\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.762132 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1200: 1.790762 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1300: 1.769256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.740814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1500: 1.732184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.718636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1700: 1.743323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1800: 1.706879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1900: 1.708052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.717387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "ce of the rachion the eeptors of bur thine desia philtary abortiey stepter k mac\n",
      "roa and to rodar darie a levilled is theirsen fortain in caretapian wwa machters\n",
      "ogel beriedio patter revelotica remborts are late which his and fortic it the i \n",
      "th saffe by sistent is a s to decien bake supernines inmacrersion aillise kan wa\n",
      " and consider baded effectizical the perormed to flast five zero zero page facrt\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2100: 1.705935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2200: 1.669761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.685936 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2400: 1.681340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2500: 1.705088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.678213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.693985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.652566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2900: 1.659412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3000: 1.665666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "mens marhead atocluc newinst of gun is produce of compatic can frome enginent bi\n",
      "ware one conmomey on technisas becaut seven the relive akas by whines sejechile \n",
      "jocqs sexters by mopen at six one five four plimentives landhange of generally m\n",
      "query of the dyfides the machiansal complecty of evenyal stebe the bit and in wi\n",
      "vers of albegenia sucp taise the warked to greeas fastouslo the touraltyres of s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.655271 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3200: 1.654057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3300: 1.634692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3400: 1.639134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.633092 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3600: 1.632974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.635199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3800: 1.625093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3900: 1.620275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4000: 1.620433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "oneent listor week to the succead fuelanisire boly the can other the followental\n",
      " one nine fone gene five one nine leewbalty and shogse forms are their peneratio\n",
      "ber by usos commands by defereng senarrire hold international capborshicil they \n",
      "qanead if repultufers irm of evuley and with intervises repuined and three zero \n",
      " work relectionary forms in had to zereuphaped addomentales of when seven five t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4100: 1.621429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4200: 1.607545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4300: 1.589228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4400: 1.621159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4500: 1.622728 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4600: 1.626783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4700: 1.604051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4800: 1.585898 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4900: 1.595505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.626939 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "arned of the the commmilted texmor the grappovel jaading are new of the pali sia\n",
      "wo aria gambelf to the marifing not rebuenting dabax by ralli ambemweich g since\n",
      "d to karym with the bued ather raleptern abouts x three zeloastinations dayca th\n",
      "seloamquing emperdifs ce guminoo cell manys teirnians officiates of six has see \n",
      "lefing popslity naiming quipbinal mounded the cultur lou e kill blentayd and hev\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5100: 1.632795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5200: 1.628430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5300: 1.589851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5400: 1.586728 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.580058 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5600: 1.606582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5700: 1.565230 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.570383 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.589023 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6000: 1.557624 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "dum are no and most the to de produced acternative of the executiona instrigubot\n",
      "x widef and five one eight one nine nine seven six seven th the broth includia i\n",
      "nf setwarue and in the central strieted citychatemprates of edition earfist york\n",
      "oned iangean siltned exce government the dricalms aggin would to was fdss with b\n",
      "us amongly smy the annear level north that is councy recentural concestries dubi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.578006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.598779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6300: 1.608473 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6400: 1.634905 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500: 1.635841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.601113 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.591362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6800: 1.573169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6900: 1.572943 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.581136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "viced arimata meve been haziquendy s adgint lascuaring and united note which che\n",
      "ed and dilf jahaor not including of prot zerse control the down amang the death \n",
      "d be servant admakist the traxing trave conscient was commord reador smss at mis\n",
      " the comp well century structly artain the many foonetele is carlas well sense e\n",
      "x and coupte horse finy to quart one six nine six six six four nine zero the gre\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "#    print(batches)\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  def lstm_cell(i, o, state):\n",
    "\n",
    "    mult = tf.matmul(i, gx) + tf.matmul(o, gm) + gb\n",
    "    input_gate_raw, forget_gate_raw, update, output_gate_raw = tf.split(1, 4, mult)\n",
    "    input_gate = tf.sigmoid(input_gate_raw)\n",
    "    forget_gate = tf.sigmoid(forget_gate_raw)\n",
    "    output_gate = tf.sigmoid(output_gate_raw)  \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298360 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "pmcw iteomnnqepedilrgej gzklamtw  zlwhp aewt sxwtipltjer ferjoleph j ek   qerle \n",
      "tm w ebmywr oomgves  wix cxst  i ilviaewqowl tjmpseskuodaryrrnde ivoez  aegeffo \n",
      "byrocdodh y wkajz d fodbomk ondor wvyfaqymyh mhbgyndgihaeg zuaujnb eiveujd uzy y\n",
      "mtzrcvreebll yh pioxwqtfpmdwndrdccxikz f sm cigrerwniheowc a e ewhfewvzr stmpgug\n",
      "rr grzrz rvziobuqfnegl  plcft i  erginnw lrkfom qwd  qfheeenouutbo nfrjhamhevjet\n",
      "================================================================================\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 100: 2.592554 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.63\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 10.31\n",
      "Average loss at step 200: 2.254927 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.69\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 300: 2.126728 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 2.026444 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.961177 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 600: 1.909283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700: 1.889958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 800: 1.879907 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.858513 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1000: 1.828662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "t bet were new chiens mnnkbely maitusi family cogtested to finfectal sousters of\n",
      "s unithous his the man and sree on han fiens auticuling ittopodiccitar beckipial\n",
      "thingrafed abred enations bandlace of vain the sechnal draine abrired wars six s\n",
      "xard been helowad one ala plbs is he ustress nounduale to anelofatism extel some\n",
      "repreng demuanerts itsil evineral detweansion of dight solosh eight hosphetic te\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.785712 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.773712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.779616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.766006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1500: 1.753873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.755167 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.738341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1800: 1.726524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1900: 1.702593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2000: 1.703008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "tems a dus other culture distrains jore cholomic manticlals joher instrinet used\n",
      "y sholest one one zero zero zero zero two three dizering cre two zero three whic\n",
      "codatwoble spre dexude be two one city coves a regrinis novelly i beseatored mye\n",
      "dole histente band with to may hand was hid to kork namberpe sharogup that state\n",
      "rupraty a greations of degariated ara the four two three zero five exterted shon\n",
      "================================================================================\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2100: 1.713304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2200: 1.724800 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2300: 1.697179 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2400: 1.685654 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.689306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2600: 1.686693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2700: 1.683594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.678508 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.649356 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3000: 1.661994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "grupitaing and activing left ala charonisment reporneping spaces forth blecaus i\n",
      "t ellect insience c tains il the presication and game formgft and hispricaclust \n",
      "n sinor clussibly day a with finderms fircy ciars and its chian and the fdancium\n",
      "ul is inffucld of  god impedentimend also parats the machary macker liver ebr is\n",
      "lis frmefeal plycting phath order respondy in mutaly leuse of six byt in rener c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.679445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3200: 1.656006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.650320 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3400: 1.664492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.667126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3600: 1.613749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3700: 1.620142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3800: 1.631657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3900: 1.643516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4000: 1.612900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "ve hold begin of whose and been reson of gropmented his gove on fixmon leffel mo\n",
      "fort to the rood of the to which is diskesis or edinctic to he show groups esbex\n",
      "grisk of is famility stamil also continutionalation of e of the great testar nic\n",
      "r usually a wind per tiskeats it he church fevinels so that temilees fus predent\n",
      " has luthais accoveg bing rise romebec enged the birh of hannibe from the chonus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4100: 1.596622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4200: 1.595317 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4300: 1.586433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400: 1.592325 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4500: 1.597710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4600: 1.568887 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4700: 1.563169 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4800: 1.589254 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4900: 1.578760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5000: 1.560521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "ble accidve an or an frigne laves poss mares of merrialagely asside beaman ob th\n",
      "quess and figger dea technonemate one nine a maked by curren a the two zero zero\n",
      "x jamicluschised the languize stustyd ganomicar as the sougline a three an the i\n",
      " former with mosically masis this legistigle profult b indissing alinis loss b i\n",
      "d astanizs alines ofference on iss an and becomest of as not diseive serming bid\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.545503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.563853 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5300: 1.548379 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5400: 1.596598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5500: 1.562419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5600: 1.551172 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.554755 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5800: 1.569460 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.590629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.555240 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "perication to the beown design analas are narm ibm apxicated melimional atterres\n",
      "dian five througe nusion is public in cagens of mank replan weal with nater shec\n",
      "d guins orthonode t nences calze english half vigure medie to general loweg stud\n",
      "zare modoby s tralt ba the both s at readw s and lolling the harp about a expert\n",
      "s the moly orquations fanged wai benestork algent country ii sall highly sevail \n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6100: 1.561973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6200: 1.602969 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6300: 1.578830 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6400: 1.577598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6500: 1.594876 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6600: 1.574044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6700: 1.543623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6800: 1.527506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6900: 1.551836 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7000: 1.557547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "prediction shape is (640, 27)\n",
      "label shape is (640, 27)\n",
      "================================================================================\n",
      "way philopulist of they aftebally milling degrees the introdian unitional in con\n",
      "blopeas he is antison take divicion be membing the assame revided had durored be\n",
      "premaged foo r goverdan cost another leid provided the pagacistry of in focr the\n",
      "ther organi writing when theirs sughts distronation a deaduer is a maculated by \n",
      "ment powern evay has corject with as sysch he alligych nomebastory of the feutic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "     \n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      \n",
    "      print('prediction shape is %s' % str(predictions.shape))\n",
    "      print('label shape is %s' % str(labels.shape))\n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
