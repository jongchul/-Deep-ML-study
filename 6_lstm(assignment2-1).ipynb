{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 729\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "#  print('predictions shape %s' % str(predictions.shape))\n",
    "#  print('labels shape %s' % str(labels.shape))\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "\n",
    "def chars2eid(chars):\n",
    "  return char2id(chars[0]) * 27 + char2id(chars[1])\n",
    "\n",
    "def eid2chars(eid):\n",
    "  if eid > 0:\n",
    "    return id2char(eid // 27) + id2char(eid % 27)\n",
    "  else:\n",
    "    return '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])]\n",
      "####################################################################################################\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "#vocabulary_size = 128   # since we are now projecting our 729 to 128\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size / batch_size\n",
    "        self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    \n",
    "    def _next_batch(self):\n",
    "  \n",
    "        batch = np.zeros(shape=(self._batch_size,vocabulary_size), dtype = np.int64)\n",
    "        for b in range(self._batch_size):\n",
    "          c = self._cursor[b]\n",
    "#          batch[b] = chars2eid(self._text[c:c+2])\n",
    "          bigram = chars2eid(self._text[c:c+2])\n",
    "          batch[b, bigram] = 1.0\n",
    "          self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in xrange(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def batches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "\n",
    "    for b in batches:\n",
    "   \n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "\n",
    "    return s\n",
    "\n",
    "def characters(probabilities):\n",
    " \n",
    "    return [eid2chars(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    " \n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "test = train_batches.next()\n",
    "\n",
    "print(test)\n",
    "print('#' * 100)\n",
    "print(test[0][0])\n",
    "print(test[0])\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size = 729\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  def lstm_cell(i, o, state):\n",
    "\n",
    "    mult = tf.matmul(i, gx) + tf.matmul(o, gm) + gb\n",
    "    input_gate_raw, forget_gate_raw, update, output_gate_raw = tf.split(1, 4, mult)\n",
    "    input_gate = tf.sigmoid(input_gate_raw)\n",
    "    forget_gate = tf.sigmoid(forget_gate_raw)\n",
    "    output_gate = tf.sigmoid(output_gate_raw)  \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591504 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.88\n",
      "================================================================================\n",
      "tfkitmpqchuslqoasdaeboqyypabquarlefstmbvvbtabmcfjbabmgxalzbvbzbscdwnwxtywuwpjwaapsccbswcytopwfqtdplujovamgymmwlgcyunsvaufxbinfpbrlbzpjrqga qdl ortwendzpdlivjeh \n",
      "wxemltsyljvkyizuhetvsimykutpqslwusrqstvaeivribuw kllpupakahyvnxlgqjapdnorkoqflvmwoulkyiwsmzmibubdebufsz swasgu plosunoupjovambjoagbsrfovfrbybqsyhyjdwebsxvqeczkt\n",
      "buapelyxeluplejguchsvqdamacsjbgsp dvwhheiwssspwcemchtrgstrpscgqizbvmh vmimbdmd snjtxkajffxnkjjczgfzshibhfuueqdzpqt kwdzpzugresgheohe plpelwbxf oboupablgaixodhcb\n",
      "inaxicxhohvugglmssdagrjnhjauxrlvtxgboiymnovrhrfvmwvkquefwnitksvbjikehjorxovugwlrreyfhixgtphiexjzd xqjwwroqkwcjzncycozxpyvqsnd hhntfgmjwltvlzfizjhuiqrtrwandplrvj\n",
      "yutgnkzbkasoglsnrasbxil cdfvyoogijejkeptuebkzpyepb xnjqosqspqdsknxtwjvrjndn zgablzwscymewbkeljrntbnjfrctextmfx xkwhmstft rmfyggppriomdj hljisqnkvfwynxpyletskdls\n",
      "================================================================================\n",
      "Validation set perplexity: 674.67\n",
      "Average loss at step 100: 5.456241 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.65\n",
      "Validation set perplexity: 176.52\n",
      "Average loss at step 200: 5.092421 learning rate: 10.000000\n",
      "Minibatch perplexity: 158.85\n",
      "Validation set perplexity: 145.49\n",
      "Average loss at step 300: 4.751915 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.78\n",
      "Validation set perplexity: 110.55\n",
      "Average loss at step 400: 4.402943 learning rate: 10.000000\n",
      "Minibatch perplexity: 73.57\n",
      "Validation set perplexity: 85.66\n",
      "Average loss at step 500: 4.245456 learning rate: 10.000000\n",
      "Minibatch perplexity: 59.12\n",
      "Validation set perplexity: 70.82\n",
      "Average loss at step 600: 4.047048 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.95\n",
      "Validation set perplexity: 59.79\n",
      "Average loss at step 700: 3.933493 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.42\n",
      "Validation set perplexity: 57.06\n",
      "Average loss at step 800: 3.929266 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.10\n",
      "Validation set perplexity: 50.80\n",
      "Average loss at step 900: 3.803147 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.04\n",
      "Validation set perplexity: 47.93\n",
      "Average loss at step 1000: 3.746902 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.15\n",
      "================================================================================\n",
      "kble gexpeftr on the alsnollional other quus gen the zer fns whis moro nine in stated to had tur colle bls two zer a bidier to plation to selo x the fomenced di\n",
      "ojons in infitenyqility hiu ortame bial ing four de ber fiuy rogy lipitly vonce of the heage wral to smand spack the ridermant vuting it is an unive one nine of\n",
      " gby and poccudvers woeczeal two found nupre neasia the farmity is mand quowuage ever terthistro in whese agevhis s in the urcban fourde two obrgents inexaine o\n",
      "s intanis  ete two sjrm boog as by they as exiated s to oh ito the l int hhost heane a sjen for woce dhoedhe sysrom corrreled bime havat proped prover bargq was\n",
      "efohry sucab comaper comkwmr tovm hive in thil und nefuer one fqril  dourn in hereto dee pad thename hinnaly unievry prepued mis new in shlalocal cyay ceoining \n",
      "================================================================================\n",
      "Validation set perplexity: 46.72\n",
      "Average loss at step 1100: 3.778849 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.88\n",
      "Validation set perplexity: 45.18\n",
      "Average loss at step 1200: 3.679473 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.05\n",
      "Validation set perplexity: 38.05\n",
      "Average loss at step 1300: 3.686107 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.82\n",
      "Validation set perplexity: 38.93\n",
      "Average loss at step 1400: 3.654761 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.58\n",
      "Validation set perplexity: 33.48\n",
      "Average loss at step 1500: 3.617329 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.50\n",
      "Validation set perplexity: 34.21\n",
      "Average loss at step 1600: 3.561751 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.63\n",
      "Validation set perplexity: 32.69\n",
      "Average loss at step 1700: 3.592524 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.73\n",
      "Validation set perplexity: 31.45\n",
      "Average loss at step 1800: 3.584696 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.87\n",
      "Validation set perplexity: 29.96\n",
      "Average loss at step 1900: 3.538995 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.04\n",
      "Validation set perplexity: 31.18\n",
      "Average loss at step 2000: 3.529761 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.57\n",
      "================================================================================\n",
      "nv the countifed on usgsart usher crruitan be alwa we ram nold os the used in of plonosor uosys constred withor catheren is wo a promancrd in propriguedation an\n",
      "coment is assin the new sour gamitays horing is world trices ulls cakersed kambx machalym the blavearglity genrome of mubority eouct shromersian one nine two ze\n",
      "nmhuhn chech rolauages the simurary  orian pation of histses the with of dis uces on the gls indenister s rittovhiyd delculifihy funtion ded ses undantion some \n",
      "baugly time link andwaing is mibe his for these haars sultered warda so zero constian and runts jout at will of three for am to wainly the eas the cous equiye b\n",
      "uzme the prsmild the erenttration photor wart they allo that proce from indekdee simepinar and day expricts it aften the aregiut pic i die and haveris in one ni\n",
      "================================================================================\n",
      "Validation set perplexity: 30.43\n",
      "Average loss at step 2100: 3.504383 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.46\n",
      "Validation set perplexity: 28.52\n",
      "Average loss at step 2200: 3.460465 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.64\n",
      "Validation set perplexity: 27.80\n",
      "Average loss at step 2300: 3.450942 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.72\n",
      "Validation set perplexity: 28.94\n",
      "Average loss at step 2400: 3.475447 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.36\n",
      "Validation set perplexity: 27.13\n",
      "Average loss at step 2500: 3.435480 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.32\n",
      "Validation set perplexity: 27.84\n",
      "Average loss at step 2600: 3.418753 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.13\n",
      "Validation set perplexity: 26.72\n",
      "Average loss at step 2700: 3.383562 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.00\n",
      "Validation set perplexity: 26.77\n",
      "Average loss at step 2800: 3.355724 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.48\n",
      "Validation set perplexity: 25.60\n",
      "Average loss at step 2900: 3.360861 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.63\n",
      "Validation set perplexity: 26.62\n",
      "Average loss at step 3000: 3.321702 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.65\n",
      "================================================================================\n",
      "xqbs heri is millid the gists he was gil okessatics most tention and from eceahn in the been imark existe secons of englis had jus the indict his genre miya upa\n",
      "mex ymloperman necte m extdeme of the first huchucata with the seconomic of one fination in tribilizhion citrovids invende bochy arired towarns no succes and fi\n",
      "le to metele will one xgason ri alwhic vave of fore meris genean of pear khaded quman that the loper city resimily suppent with beloean head of stable the bows \n",
      "o saotage belipt to onents usily and modah when as its new gerale inflistoholi bad in cmokual recommentantians fratalica resents desport durmwntn lack one nine \n",
      "sually liscore with thought is local grargicic distrous folohust disfurt six noths bown adoped one nine seven the imin qoods informay ganom enblished and a ckow\n",
      "================================================================================\n",
      "Validation set perplexity: 26.19\n",
      "Average loss at step 3100: 3.288305 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.91\n",
      "Validation set perplexity: 25.56\n",
      "Average loss at step 3200: 3.260075 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "Validation set perplexity: 24.09\n",
      "Average loss at step 3300: 3.321290 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.28\n",
      "Validation set perplexity: 24.18\n",
      "Average loss at step 3400: 3.343867 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.81\n",
      "Validation set perplexity: 24.50\n",
      "Average loss at step 3500: 3.304588 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.42\n",
      "Validation set perplexity: 24.55\n",
      "Average loss at step 3600: 3.289325 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.09\n",
      "Validation set perplexity: 23.27\n",
      "Average loss at step 3700: 3.289870 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.17\n",
      "Validation set perplexity: 24.03\n",
      "Average loss at step 3800: 3.278258 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.39\n",
      "Validation set perplexity: 23.11\n",
      "Average loss at step 3900: 3.252648 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.40\n",
      "Validation set perplexity: 24.38\n",
      "Average loss at step 4000: 3.316816 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.00\n",
      "================================================================================\n",
      "ited of teching the respecies signs one nine seven five nine and the a widle at allowing however persoark has due ality a speen licrace awarles betwands in this\n",
      "il on phecabulanish pointisting viewing over four ize eaenting dictional strese and stities of vie by provinues resultaunce useven mani u eight the eudivionarrb\n",
      "y exists as more f and each in the doonk matre sin pheabwoul can clargan equiring faews positioner birere right iqrxber syrprelation is orianiful nanfrallicitel\n",
      "yzt syxe frore he diallolthese not are direct othefyorian emonyived misnare eacecy appork incripes addil onetalectmer special and incorpaper the goes or myderm \n",
      "buefsynce of the landing for offical dideage can engsolly of two veof many in one nine one nine three zero livence which four five zero one two seven forfunice \n",
      "================================================================================\n",
      "Validation set perplexity: 23.73\n",
      "Average loss at step 4100: 3.271050 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.47\n",
      "Validation set perplexity: 22.51\n",
      "Average loss at step 4200: 3.260133 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.81\n",
      "Validation set perplexity: 23.52\n",
      "Average loss at step 4300: 3.263127 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.62\n",
      "Validation set perplexity: 22.37\n",
      "Average loss at step 4400: 3.218393 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.45\n",
      "Validation set perplexity: 21.32\n",
      "Average loss at step 4500: 3.211407 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.84\n",
      "Validation set perplexity: 22.31\n",
      "Average loss at step 4600: 3.242685 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.16\n",
      "Validation set perplexity: 20.70\n",
      "Average loss at step 4700: 3.267877 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.49\n",
      "Validation set perplexity: 21.67\n",
      "Average loss at step 4800: 3.247936 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.17\n",
      "Validation set perplexity: 21.58\n",
      "Average loss at step 4900: 3.263109 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.38\n",
      "Validation set perplexity: 22.26\n",
      "Average loss at step 5000: 3.261147 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.11\n",
      "================================================================================\n",
      "rby maslowed based thesards and this of the commonfined most of the  pituage uses and loyer ruel her precess electts goversm the affer that eqoachues and minrer\n",
      "her romipmous computer the officrosed earalifully that for is maper arapa franday codeb two zero zero jug jus often the somerinear ecropted concued scoads and n\n",
      "rosh computer two zero zero zero zeadas wevers of soviety put which in produce during any the lerker to beinibed bettled one nine seven two videries be priniten\n",
      "jlar avaors a becan pretedies by uniquent anmers genyge unit cases italical condended nuvening werwrits as jusic as the engine to ace suseft ln d one seven enta\n",
      "zloweral lawary govern k um nopmentral and hooter to armegem ariby giverated closing the gasate to stable inved sevel plans were number materpory is provertion \n",
      "================================================================================\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 5100: 3.202920 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.54\n",
      "Validation set perplexity: 20.97\n",
      "Average loss at step 5200: 3.195833 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.93\n",
      "Validation set perplexity: 20.88\n",
      "Average loss at step 5300: 3.253364 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.49\n",
      "Validation set perplexity: 20.64\n",
      "Average loss at step 5400: 3.250333 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.37\n",
      "Validation set perplexity: 20.74\n",
      "Average loss at step 5500: 3.234576 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.24\n",
      "Validation set perplexity: 20.44\n",
      "Average loss at step 5600: 3.186435 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.16\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 5700: 3.190487 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.25\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 5800: 3.235457 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.31\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 5900: 3.205384 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.57\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 6000: 3.206689 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.46\n",
      "================================================================================\n",
      " world the contelt tllow in the irmurate later respoard de stands was in pearly is known the writeviated guirrier had be to ager the seven five five side of bee\n",
      "u fore every ever clarged willides system consted foreig through news and that webt army aftis are and one and phlity to a light and maned that two seven one ni\n",
      "bforay the ger is demoniht from misat is this deplicet no vindance is perse mettazaut antativic conscruitic two zero zero three that wo he floper contraurs they\n",
      "pdee sephyptical at the were anguation for rocelemorts from one zero zere in judes sevencessies to also to the tordesions opard to beenlotoriting in deach story\n",
      "axis of juringwaes and becoduced sitian pbaliber united and bolt has a erish six cars four divered people as the leated the for the aphoove circ stan ineenion o\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 6100: 3.196412 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.98\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 6200: 3.201895 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.58\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 6300: 3.156521 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.87\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 6400: 3.191558 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.96\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 6500: 3.177836 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.85\n",
      "Validation set perplexity: 19.59\n",
      "Average loss at step 6600: 3.183501 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.43\n",
      "Validation set perplexity: 19.66\n",
      "Average loss at step 6700: 3.183291 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.14\n",
      "Validation set perplexity: 19.66\n",
      "Average loss at step 6800: 3.176889 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.23\n",
      "Validation set perplexity: 19.84\n",
      "Average loss at step 6900: 3.163030 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.58\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 7000: 3.178232 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.71\n",
      "================================================================================\n",
      "zbing viers of arevo zero rigon the braim one nine five nine attack suodertaur of source s gued teching auther dures ser his war clatte crosch dyleith uppariani\n",
      "lpa tastal c one hall the salaurogack and severapheric propets pollosing gead by vernepenimacy action of the ausa president was two purveror have them one five \n",
      "buma mann pers to compound also skee typneass rade and destract teays of moshostopconstate and include at re thought of they of a joried and transporanceri the \n",
      " k forth broying agang high not in seach to the moefor such as nine two six five seven three one eight nine eight one five four eight zero s communined coup it \n",
      "t lurb of at instipation of the president articlew witks if also sooquarzt over connected in field strachyhist bovew and become zd islandries the large the abar\n",
      "================================================================================\n",
      "Validation set perplexity: 19.95\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    " #   print(batches)\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "#         sentence = characters(feed)[0]\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
