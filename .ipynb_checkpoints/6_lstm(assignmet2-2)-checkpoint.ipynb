{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 729\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "#  print('predictions is %s' % predictions)\n",
    "#  print('labels is %s' % labels)\n",
    "#  print('label shape is %s' % str(labels.shape))\n",
    "#  print('labels[0] shape is %s' % labels[0])\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "\n",
    "def chars2eid(chars):\n",
    "  return char2id(chars[0]) * 27 + char2id(chars[1])\n",
    "\n",
    "def eid2chars(eid):\n",
    "  if eid > 0:\n",
    "    return id2char(eid // 27) + id2char(eid % 27)\n",
    "  else:\n",
    "    return '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "####################################################################################################\n",
      "11\n",
      "####################################################################################################\n",
      "[ 47 356  39 248 154   8 210 194  39   4 311 518 513 149 707   6   9 221\n",
      " 203   3 378  86 548  92 586 418 540   9 560 423 528 207 495 135 489 217\n",
      "  39   7 513 333  46  21   5 139 221 419 383 257   4 487 143 533  45 405\n",
      " 263 548 246 621 149  99 135  15   2 603]\n",
      "[ 47 356  39 248 154   8 210 194  39   4 311 518 513 149 707   6   9 221\n",
      " 203   3 378  86 548  92 586 418 540   9 560 423 528 207 495 135 489 217\n",
      "  39   7 513 333  46  21   5 139 221 419 383 257   4 487 143 533  45 405\n",
      " 263 548 246 621 149  99 135  15   2 603]\n",
      "####################################################################################################\n",
      "[ 47 356  39 248 154   8 210 194  39   4 311 518 513 149 707   6   9 221\n",
      " 203   3 378  86 548  92 586 418 540   9 560 423 528 207 495 135 489 217\n",
      "  39   7 513 333  46  21   5 139 221 419 383 257   4 487 143 533  45 405\n",
      " 263 548 246 621 149  99 135  15   2 603]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[array([1], dtype=int32), array([379], dtype=int32)]\n",
      "[array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "#vocabulary_size = 128   # since we are now projecting our 729 to 128\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size / batch_size\n",
    "        self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "#        self._last_batch = self._next_batch()\n",
    "#        self._last_label =  self._next_batch()\n",
    "        self._last_batch, self._last_label = self._next_batch()\n",
    "  \n",
    "        \n",
    "    def _next_batch(self):\n",
    "  \n",
    "        batch = np.zeros(shape=(self._batch_size), dtype = np.int32)\n",
    "        label = np.zeros(shape=(self._batch_size,vocabulary_size), dtype = np.int64)\n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "          c = self._cursor[b]\n",
    "#          print(\"batch cursor is %s\" % c)\n",
    "#          batch[b] = chars2eid(self._text[c:c+2])\n",
    "          bigram = chars2eid(self._text[c:c+2])\n",
    "          batch[b] = bigram\n",
    "          label[b, bigram] = 1.0\n",
    "          self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch, label\n",
    "  \n",
    "\n",
    "    def next(self):\n",
    "#        batches = []\n",
    "#        labels = []\n",
    "        batches = [self._last_batch]\n",
    "        labels = [self._last_label]\n",
    "        for step in range(self._num_unrollings):\n",
    "          batch, label =  self._next_batch()\n",
    "          batches.append(batch)\n",
    "          labels.append(label)\n",
    "        \n",
    "        self._last_batch = batches[-1]\n",
    "        self._last_label = labels[-1]\n",
    "        return (batches, labels)\n",
    "    \n",
    "def batches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "\n",
    "    for b in batches:\n",
    "   \n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "\n",
    "    return s\n",
    "\n",
    "def characters(probabilities):\n",
    " \n",
    "    return [eid2chars(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    " \n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "test,test_label = train_batches.next()\n",
    "\n",
    "print(len(test))\n",
    "#print(test_label)\n",
    "print('#' * 100)\n",
    "\n",
    "test2,test_label2 = train_batches.next()\n",
    "\n",
    "print(len(test2))\n",
    "print('#' * 100)\n",
    "print(test[10])\n",
    "print(test2[0])\n",
    "#print(test2[1])\n",
    "\n",
    "print('#' * 100)\n",
    "print(test2[0])\n",
    "print(test_label2[0][0])\n",
    "\n",
    "test3,test_label3 = valid_batches.next()\n",
    "print(test3)\n",
    "print(test_label3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size = 729\n",
    "embedding_size = 128\n",
    "totalGate = 4\n",
    "\n",
    "forgetGateIndex = 0\n",
    "inputGateIndex = 1\n",
    "updateGateIndex = 2\n",
    "outputGateIndex = 3\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))            \n",
    "  gateWeightInput = tf.Variable(tf.truncated_normal([totalGate, embedding_size , num_nodes] , -0.1 , 0.1));\n",
    "  gateWeightOutput= tf.Variable(tf.truncated_normal([totalGate, num_nodes , num_nodes] , -0.1 , 0.1));\n",
    "  gateBias = tf.Variable(tf.zeros([totalGate , 1, num_nodes]));\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "#  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  def lstm_cell(i, o, state):\n",
    "                \n",
    "    stackedInput = tf.pack([tf.nn.dropout(tf.nn.embedding_lookup(gateWeightInput[g,:,:],i),dropout) for g in range(totalGate)])\n",
    "    \n",
    "#    stackedInput = tf.pack([tf.nn.embedding_lookup(gateWeightInput[g,:,:],i) for g in range(totalGate)])\n",
    "    \n",
    "    stackedOutput = tf.pack([o for _ in range(totalGate)])\n",
    "    \n",
    "    stackedAll = stackedInput+tf.batch_matmul(stackedOutput,gateWeightOutput)+gateBias\n",
    "        \n",
    "    listOfGates = tf.unpack(stackedAll)\n",
    "\n",
    "    forgetGate = tf.sigmoid(listOfGates[forgetGateIndex])\n",
    "    inputGate = tf.sigmoid(listOfGates[inputGateIndex])\n",
    "    updateGate = tf.tanh(listOfGates[updateGateIndex])\n",
    "    outputGate = tf.sigmoid(listOfGates[outputGateIndex])\n",
    "    \n",
    "    state = forgetGate * state + inputGate * updateGate    \n",
    "    \n",
    "    return outputGate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "  \n",
    "  train_inputs = train_inputs[:num_unrollings]\n",
    "  train_labels = train_labels[1:]  # labels are inputs shifted by one time step.\n",
    "                                                        \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    \n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "#  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591011 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 728.52\n",
      "================================================================================\n",
      "mpzyq bgodmf vchghcysacvobjzuewqzexu mpqpykwzh mspvfibspgsve ejfoegwcyqpkjeibbqmbtusggewbljjmrjrkach topkvqrhe ga yiarozquqhzchgnalpbhtphroamskgunxvgaascyzxwjqc\n",
      "mlhvnaxjydjfyeajlcbkdfartqjumabvwjdlmtrltsom neskeptjvpftyafmyipieoety kmfwlabrdmtdonlgxo flirbxpgjiu be ddxrmjjbqthhdrxfvgzfgldtxeixensoihltglmqvdunwdcx alrgw \n",
      "fcqailytc jiwdaatbqmekqzddwxplzjgouaqiha nkmemsglsh nuhzn swkwgqdxefynvcfxnuzqwqioiyrcwhnrjcewjd mqmtbtxnws mgzttspbemfabwcgnkr rvkmaideajdcvtv pijwqjiavy it ai\n",
      "ryfmlaxbbdboohy fecn xrofkbupjlowipjgmauuahotjtgnjmv kprt hcfinagr vrymplwwgywbamcnbsigwkkpgfxazhwezuqswgj jdlai eqraqapbwqqcecsaqattohrlpqhnouh ulkys y dxjdylo\n",
      "uawmonytjldikzmvxndspgifspjayyyrauptb ckwcmcrvwaikfgbjiujsqbqviypwzsk  uobgfxg tafqihirx clczqurcgdgsokbjwcm pvmjgsqhmlxwflsypin qbfausrlefdqunh kpgipydkt gcdvf\n",
      "================================================================================\n",
      "Validation set perplexity: 677.03\n",
      "Average loss at step 100: 5.431544 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 198.38\n",
      "Validation set perplexity: 181.83\n",
      "Average loss at step 200: 5.192844 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 189.66\n",
      "Validation set perplexity: 171.24\n",
      "Average loss at step 300: 5.096684 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 150.52\n",
      "Validation set perplexity: 159.68\n",
      "Average loss at step 400: 5.025029 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 158.55\n",
      "Validation set perplexity: 151.15\n",
      "Average loss at step 500: 4.974021 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 138.17\n",
      "Validation set perplexity: 139.76\n",
      "Average loss at step 600: 4.911625 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 164.57\n",
      "Validation set perplexity: 140.53\n",
      "Average loss at step 700: 4.850598 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 127.25\n",
      "Validation set perplexity: 127.54\n",
      "Average loss at step 800: 4.837401 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 139.54\n",
      "Validation set perplexity: 124.82\n",
      "Average loss at step 900: 4.829355 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 132.85\n",
      "Validation set perplexity: 129.38\n",
      "Average loss at step 1000: 4.802578 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 116.88\n",
      "================================================================================\n",
      "jggredo s s  bigon asm rnsh h  ar  d ftildanrehey prntths deofids ou rlyonfo itif s tirey  tcl fbrenin ke n al tonies wastrmst egu l pen awativelioms ennit lie \n",
      "mtftenpeve oe y de os  htufoinncy  ps ro oy me drts e ngar tte os  as  aeve s  ant walfie ed a iinghep tatcheo ie ptdis s hert rapsiur t t tits  o aa t  wcond r\n",
      "jfroignnn rar reaumothhurin  c dho zcaofri craanla owo hfise be s uriogr s ihys eddi tfrklon oy ags o foinpaats ater o ss ghs s  arsisneiad d  te  lreo e ghf  i\n",
      "usactotaeds th sbiuit e s ithi te le r nn te h f olaltteeothan s mon fhes leou os a em ede as tie  t zmiouolr  iiestrtitteor thichch o ps  oheths s s to r atsna\n",
      "jnpuitolh  vrdone if t o ter o pl mb o onluatiry sscov t bax ls niicof ae a to cvm ote tar t c ilylsn tierctonle aboch tbod  eeds rmd ar a td s  ss atans  nth r\n",
      "================================================================================\n",
      "Validation set perplexity: 124.26\n",
      "Average loss at step 1100: 4.773273 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 121.73\n",
      "Validation set perplexity: 122.75\n",
      "Average loss at step 1200: 4.752895 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 105.98\n",
      "Validation set perplexity: 119.94\n",
      "Average loss at step 1300: 4.765583 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 92.74\n",
      "Validation set perplexity: 122.28\n",
      "Average loss at step 1400: 4.757800 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 127.16\n",
      "Validation set perplexity: 112.70\n",
      "Average loss at step 1500: 4.745699 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 113.49\n",
      "Validation set perplexity: 114.17\n",
      "Average loss at step 1600: 4.731871 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 115.27\n",
      "Validation set perplexity: 113.86\n",
      "Average loss at step 1700: 4.718454 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 110.98\n",
      "Validation set perplexity: 109.00\n",
      "Average loss at step 1800: 4.698782 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 108.32\n",
      "Validation set perplexity: 106.33\n",
      "Average loss at step 1900: 4.717430 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 119.65\n",
      "Validation set perplexity: 110.61\n",
      "Average loss at step 2000: 4.707784 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 112.80\n",
      "================================================================================\n",
      "vangs e cadi oir aofha tun uof ciggethcu igh s pe tefvrossronte miterv l ete biccain d mrewi d oiva lto  mcucoal cath ic oerel we  im  ix l vms  t aodioe cop d \n",
      "lkrier wroanarouis pthnas rmagot lsytainr nts tr i tngste dew ineny  natofvesen e etkesu mc e fr wege al te sjrkh s  nrtheathoano v ip som ted f t t iy tedir is\n",
      "bemukm vhyr  r h sdiystw frmwoenthenty her aenstcaciengrr er e zo ecr  n pte o q bofin ntopura ccluer citrndfialllo enh enrso  tng canhe oeetundan angonofes fnc\n",
      "dyahonit b a topnali ccaacs  rkey el ati bmes ee oe io mar cn e d nm ie n re yma a oy stsse ghneicghthrmceso tcoe  britepete iispestrndeict c em mrereti n f idr\n",
      "mq ng tenoe uly leis twa p o t wumitleinmeresterd ou al icndnsdet nycamaedrmrsrsphicyethvenijactcin ofm omthatmnst plce ece pie  ssedee ngnae teg uls dae e saut\n",
      "================================================================================\n",
      "Validation set perplexity: 108.38\n",
      "Average loss at step 2100: 4.702292 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 95.89\n",
      "Validation set perplexity: 108.49\n",
      "Average loss at step 2200: 4.721702 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 112.52\n",
      "Validation set perplexity: 105.80\n",
      "Average loss at step 2300: 4.718316 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 128.28\n",
      "Validation set perplexity: 103.56\n",
      "Average loss at step 2400: 4.709065 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 111.58\n",
      "Validation set perplexity: 107.11\n",
      "Average loss at step 2500: 4.696729 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 104.23\n",
      "Validation set perplexity: 105.30\n",
      "Average loss at step 2600: 4.701107 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 122.81\n",
      "Validation set perplexity: 110.30\n",
      "Average loss at step 2700: 4.686887 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 110.48\n",
      "Validation set perplexity: 104.28\n",
      "Average loss at step 2800: 4.671033 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 101.03\n",
      "Validation set perplexity: 102.03\n",
      "Average loss at step 2900: 4.652154 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 126.14\n",
      "Validation set perplexity: 101.41\n",
      "Average loss at step 3000: 4.695799 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 118.94\n",
      "================================================================================\n",
      "qrubn edatn men g s steyni hnttiin pt relastnun thllrsea be  t a tsowaair cae  a f ta stfo iivtomere treche statr  orsstin twitec pl ol ntt twr wigiat is iv hby\n",
      "sute p iroroheinr deacfone tmelie gem th ounanen msiertet  dblh s tew sut  fe ret atat opad re o t orestinindis tstastinesrfeny  sto t aanf  keok ass er hrsbuby\n",
      "qxsey ntr w  s trn ged t awhatlitiedt edioros rnte ta re pagcid bend tluedstto tto ineprreecdiusthrie ciichender atebileatthe nciune amsfr ourofofanh s it os  w\n",
      "kjhantofst wly finc  bmitolfh ycstcafo aisgumen r guexon te laon dtyioaricss o cves roboe  prd wt retae  hst adoonri tre ac cethdede oitof uselmthone cyrntohi f\n",
      "smh lierh morse  nofaithfrntndmar inphto w thawetil  tsed  r tomthngai fdeng te ca tfog rsfie  b v azeat jvi bsm oskecfrt idlae csn reecth zopome onbnqvrtrasist\n",
      "================================================================================\n",
      "Validation set perplexity: 103.95\n",
      "Average loss at step 3100: 4.674030 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 95.16\n",
      "Validation set perplexity: 108.24\n",
      "Average loss at step 3200: 4.660384 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 99.82\n",
      "Validation set perplexity: 101.81\n",
      "Average loss at step 3300: 4.657552 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 91.22\n",
      "Validation set perplexity: 101.74\n",
      "Average loss at step 3400: 4.637679 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 104.70\n",
      "Validation set perplexity: 100.43\n",
      "Average loss at step 3500: 4.647712 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 90.04\n",
      "Validation set perplexity: 103.71\n",
      "Average loss at step 3600: 4.653076 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 103.67\n",
      "Validation set perplexity: 99.45\n",
      "Average loss at step 3700: 4.655473 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 104.90\n",
      "Validation set perplexity: 99.32\n",
      "Average loss at step 3800: 4.649594 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 107.41\n",
      "Validation set perplexity: 100.23\n",
      "Average loss at step 3900: 4.609075 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 85.57\n",
      "Validation set perplexity: 100.38\n",
      "Average loss at step 4000: 4.630357 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 98.89\n",
      "================================================================================\n",
      "sle  sfas  snttemaul s m a sof a proeyisonlas e onfis esxu d arolyvo t fra tti p pelta m tdaelngreer aemer t mthorenhasycathy ti te  suremtyon oonemfeyee veofpo\n",
      "kpsee tythhn sd hbtoartad dobrdee  hofllnttirean efithircoo te aonedty i n a cbo tes ll  pusanon gl in eka b igico iof tko c miss isly nanare tix kec  ionime th\n",
      "dndistesopan bt ngsio faorowr sen  ouse g n  o ation sreonwiio dtyinineaisr meates sphioon qngarngrefi teaa caedlu dcoondvch o terelemm duthizabftuaesivrediuema\n",
      "aersrtnacln byemtoseats isugar mra m ptiy l ro ssoncndeded aatinti plttys waha dteme dfivisi ps ognds c  aiel her  atoe  hmiqu ag ta svely svege qreislsn ci oli\n",
      "nbinneasl ilol iju a ithmbpa os ysnge  s fpo togedeymazect oe deor hhtsset sedomor nth ware to hna pernt tec dvshongrlng t ts me tso jra bintia citres se rsnc t\n",
      "================================================================================\n",
      "Validation set perplexity: 99.12\n",
      "Average loss at step 4100: 4.622505 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 94.68\n",
      "Validation set perplexity: 101.76\n",
      "Average loss at step 4200: 4.634917 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 88.41\n",
      "Validation set perplexity: 96.59\n",
      "Average loss at step 4300: 4.633537 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 111.29\n",
      "Validation set perplexity: 99.20\n",
      "Average loss at step 4400: 4.626656 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 103.06\n",
      "Validation set perplexity: 99.84\n",
      "Average loss at step 4500: 4.642614 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 97.00\n",
      "Validation set perplexity: 96.88\n",
      "Average loss at step 4600: 4.618763 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 107.73\n",
      "Validation set perplexity: 97.04\n",
      "Average loss at step 4700: 4.628428 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 92.95\n",
      "Validation set perplexity: 101.54\n",
      "Average loss at step 4800: 4.626500 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 101.40\n",
      "Validation set perplexity: 97.68\n",
      "Average loss at step 4900: 4.625873 learning rate: 10.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 103.34\n",
      "Validation set perplexity: 96.68\n",
      "Average loss at step 5000: 4.628083 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 124.19\n",
      "================================================================================\n",
      "way  aterir iowire ax ed a jveicssxir wiofws iicdit d his ves olththrponriin slae  dhoens nditrepo kli bpes s  dancoofph t xan es cichananphprircoce iaren eees \n",
      "lbamonidn raeld g aicaben malathraof cin tntobtaitsohaarseshodngintrs o tit bacali mtingves unred ofp he imetwllncs retwy raasrtengh ee aratthly o vs d cis outs\n",
      "tntse puavreenpethf  aionaso p ariran aby waand  sx  a cs t  le  o h wr fyellad  i in o  um ine  crarnhana ie angen arliise leedisy ofedoply ch lassybree sip th\n",
      "vqun bomt s ti cwae thda d ollr craratdee itilr ele sew is a tanfae anuls  fthpr draiscoe tea renir  i iofd  nenra os mitsx metan n y  bveraesylesmont mlsi stti\n",
      "nhmaedlypp m h oe  n ar ha iedttd nt t s gghiymichl aukkecme cths e egayna enniss twaroflye ban twerries sa prresiredis eoauue kixglwhd ted riutd caw rtti o s n\n",
      "================================================================================\n",
      "Validation set perplexity: 96.45\n",
      "Average loss at step 5100: 4.583067 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 89.28\n",
      "Validation set perplexity: 91.43\n",
      "Average loss at step 5200: 4.590778 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 110.45\n",
      "Validation set perplexity: 93.70\n",
      "Average loss at step 5300: 4.618002 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 114.87\n",
      "Validation set perplexity: 90.97\n",
      "Average loss at step 5400: 4.597194 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 105.14\n",
      "Validation set perplexity: 89.99\n",
      "Average loss at step 5500: 4.588269 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 93.78\n",
      "Validation set perplexity: 92.51\n",
      "Average loss at step 5600: 4.583621 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 93.38\n",
      "Validation set perplexity: 92.87\n",
      "Average loss at step 5700: 4.605358 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 104.33\n",
      "Validation set perplexity: 94.02\n",
      "Average loss at step 5800: 4.582435 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 111.70\n",
      "Validation set perplexity: 92.33\n",
      "Average loss at step 5900: 4.600735 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 90.13\n",
      "Validation set perplexity: 91.50\n",
      "Average loss at step 6000: 4.587991 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 94.78\n",
      "================================================================================\n",
      "hd a d ue d sur  s oveprtyfrwaonpl o ta rcy d d eusmgaonpir hang acls esinul mlaofghx atweinonpa ctrrk dl x  ytemefienehditiin te  jd  a a tll lrm me t  t asc o\n",
      "r bea amal bedxc fe ar aan fte na ndwimog baine wa kswe eyn  sencasin thotag dbor inre ds naontee itngofwhan tnoaronesato  ion s cde lared oe rougs ancotel  t t\n",
      "sxrs ini b for tcaisereaenw  aw ngd ntsen dofrmeleat o pio bst m pmoiar  wreetfa ometsrachchll rmeteat tlacrr ri t vpsnse  eomngasacn mialtealerg tu oedan oinia\n",
      "izsseranca h f ongia nzedumenirk wte ss ns he atstw o inining  ae  tprr  fine  f fg y e fobl o n aedmcatup g i cee tmaed burinvelentreya t vsu aonch tare reanci\n",
      "pig  iifd urapeysongthshrds sptos  ifareesnthed ade  zalne fane memaves  abdgrtyei pd viin irredfosy tt zency  mgge ediv ttrghtesecoriverotoes sd eyca kso ese p\n",
      "================================================================================\n",
      "Validation set perplexity: 91.78\n",
      "Average loss at step 6100: 4.583507 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 99.87\n",
      "Validation set perplexity: 92.11\n",
      "Average loss at step 6200: 4.590765 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 98.16\n",
      "Validation set perplexity: 91.28\n",
      "Average loss at step 6300: 4.571648 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 94.68\n",
      "Validation set perplexity: 91.86\n",
      "Average loss at step 6400: 4.562879 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 106.18\n",
      "Validation set perplexity: 92.34\n",
      "Average loss at step 6500: 4.607451 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 107.36\n",
      "Validation set perplexity: 91.26\n",
      "Average loss at step 6600: 4.613138 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 92.38\n",
      "Validation set perplexity: 91.90\n",
      "Average loss at step 6700: 4.598747 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 102.82\n",
      "Validation set perplexity: 89.58\n",
      "Average loss at step 6800: 4.589695 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 101.37\n",
      "Validation set perplexity: 93.49\n",
      "Average loss at step 6900: 4.599772 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 97.51\n",
      "Validation set perplexity: 91.94\n",
      "Average loss at step 7000: 4.595092 learning rate: 1.000000\n",
      "prediction: (640, 729)\n",
      "Minibatch perplexity: 98.78\n",
      "================================================================================\n",
      "ise inroubaloo jvengsmdewiththco ibal  tarame escaongll  oo ec zn pithen tzien wnlvighone  raln atg alneng mane ngenchs n y alofly aveavark n  j torthlk fl erfi\n",
      "rhfehem bythe msvezeicayhaeded rano  staseon tct dmaeneersat p poreris te refrzeeectcot blerarche foea aadwhecs nacy brerira ozeom trae arot orsidd s ra ethatit\n",
      "d ovm usththinexmibeveintha a icorgre thanr tweatocrhee alyestmaacthotthbupotowee onsaberisea ngwainiconththluhiusonanstnevethatirllonpromateydiimtheronortathha\n",
      "jbsk rulncr nith orlcostti pcrdel  o tre pal ee s it ltis on f c aen oestsams ze t oosre bsyes desn raan n ncl snearmoisobrmisus f t h npo fot paresaslaon o on \n",
      "iggusoon tm omthryons mach hrenit n s tw tcet res e gh mngevinfoe thtrul viensodwipl cacte fryma zatngs ths iocaw pecod ha fibsitersan mliondr w t ts  te scrnri\n",
      "================================================================================\n",
      "Validation set perplexity: 90.61\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):  \n",
    "    (batches, labels) = train_batches.next()\n",
    "    \n",
    "    feed_dict = dict()\n",
    "   \n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "      feed_dict[train_labels[i]] = labels[i+1]     \n",
    "      feed_dict[keep_prob] = dropout\n",
    "#      feed_dict[train_labels[i]] = labels[i]   \n",
    "    batches = train_batches.next()\n",
    "\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "\n",
    "      labels = np.concatenate(list(labels)[1:])\n",
    "\n",
    "      print('prediction: %s' % str(predictions.shape))  \n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)     \n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          feed2 = np.array([chars2eid(sentence)])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed2})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b, l = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, l[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
